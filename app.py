import streamlit as st
import cv2
import numpy as np
import os
import tempfile
import torch
from torch import nn
import mediapipe as mp
from PIL import Image
import time

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ‰‹åŠ¿è¯†åˆ«åº”ç”¨",
    page_icon="ğŸ‘‹",
    layout="wide"
)

# æ ‡é¢˜
st.title("ğŸ¬ æ‰‹åŠ¿è¯†åˆ«è§†é¢‘åˆ†æåº”ç”¨")
st.markdown("ä¸Šä¼ MP4è§†é¢‘æ–‡ä»¶ï¼Œç³»ç»Ÿå°†æ£€æµ‹å¹¶è¯†åˆ«æ‰‹åŠ¿åŠ¨ä½œ")

# åˆå§‹åŒ–MediaPipe
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

def mediapipe_detection(image, model):
    """å¤„ç†å›¾åƒå¹¶æ£€æµ‹å…³é”®ç‚¹"""
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image.flags.writeable = False
    results = model.process(image)
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    return image, results

def draw_styled_landmarks(image, results):
    """ç»˜åˆ¶å…³é”®ç‚¹å’Œè¿æ¥çº¿"""
    if results.pose_landmarks:
        mp_drawing.draw_landmarks(
            image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,
            mp_drawing.DrawingSpec(color=(0,0,255), thickness=1, circle_radius=1),
            mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1)
        )
    if results.left_hand_landmarks:
        mp_drawing.draw_landmarks(
            image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
            mp_drawing.DrawingSpec(color=(0,0,255), thickness=1, circle_radius=2),
            mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1)
        )
    if results.right_hand_landmarks:
        mp_drawing.draw_landmarks(
            image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
            mp_drawing.DrawingSpec(color=(0,0,255), thickness=1, circle_radius=2),
            mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1)
        )

def extract_keypoints(results):
    """æå–å…³é”®ç‚¹æ•°æ®"""
    pose = np.array([[res.x, res.y, res.z, res.visibility] 
                     for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)
    lh = np.array([[res.x, res.y, res.z] 
                   for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    rh = np.array([[res.x, res.y, res.z] 
                   for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)
    
    return np.concatenate([pose, lh, rh])

# å®šä¹‰LSTMæ¨¡å‹ç±»
class CustomLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(CustomLSTM, self).__init__()
        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, 64)
        self.fc2 = nn.Linear(64, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 32)
        self.fc5 = nn.Linear(32, 32)
        self.output_layer = nn.Linear(32, num_classes)
    
    def forward(self, x):
        x, _ = self.lstm1(x)
        x, _ = self.lstm2(x)
        x, _ = self.lstm3(x)
        x = torch.relu(self.fc1(x[:, -1, :]))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = torch.relu(self.fc4(x))
        x = torch.relu(self.fc5(x))
        x = self.output_layer(x)
        return x


model_path = "model.pth"
    

# æ‰‹åŠ¿ç±»åˆ«åˆ—è¡¨
gestures = np.array(['polis','nasi','abang','apa','hari','ribut','pukul','beli','emak','perlahan'])

# ä¸»å†…å®¹åŒº
uploaded_file = st.file_uploader("ğŸ“ ä¸Šä¼ MP4è§†é¢‘æ–‡ä»¶", type=['mp4', 'avi', 'mov'])

if uploaded_file is not None:
    # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“‹ æ–‡ä»¶ä¿¡æ¯")
        file_name = uploaded_file.name
        file_size = uploaded_file.size / (1024 * 1024)  # è½¬æ¢ä¸ºMB
        
        st.write(f"**æ–‡ä»¶å:** {file_name}")
        st.write(f"**æ–‡ä»¶å¤§å°:** {file_size:.2f} MB")
        st.write(f"**æ£€æµ‹åˆ°çš„æ‰‹åŠ¿ç±»åˆ«æ•°:** {len(gestures)}")
        
        # æ˜¾ç¤ºè§†é¢‘ä¿¡æ¯
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            video_path = tmp_file.name
        
        # è¯»å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
        cap = cv2.VideoCapture(video_path)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        duration = total_frames / fps if fps > 0 else 0
        
        st.write(f"**è§†é¢‘åˆ†è¾¨ç‡:** {width} x {height}")
        st.write(f"**å¸§ç‡:** {fps} FPS")
        st.write(f"**æ€»å¸§æ•°:** {total_frames}")
        st.write(f"**æ—¶é•¿:** {duration:.2f} ç§’")
        
        cap.release()
    
    # å¼€å§‹å¤„ç†æŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹å¤„ç†è§†é¢‘", type="primary"):
        with st.spinner("æ­£åœ¨å¤„ç†è§†é¢‘ï¼Œè¯·ç¨å€™..."):
            # è¿›åº¦æ¡
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # å®ä¾‹åŒ–æ¨¡å‹
            try:
                input_size = 258
                hidden_size = 64
                num_classes = len(gestures)
                
                # åˆ›å»ºæ¨¡å‹å®ä¾‹
                model = CustomLSTM(input_size, hidden_size, num_classes)
                
                model = torch.load(model_path, weight_only=False)
                
                model.eval()
                
            except Exception as e:
                st.error(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
                st.info("ç»§ç»­ä½¿ç”¨é»˜è®¤æ¨¡å‹è¿›è¡Œæ¼”ç¤º")
                # åˆ›å»ºé»˜è®¤æ¨¡å‹
                model = CustomLSTM(258, 64, len(gestures))
                model.eval()
            
            # å¤„ç†è§†é¢‘
            cap = cv2.VideoCapture(video_path)
            
            # å‡†å¤‡è¾“å‡ºè§†é¢‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if save_output:
                output_path = f"processed_{file_name}"
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            
            # åˆå§‹åŒ–å˜é‡
            sequence = []
            predictions_history = []  # å­˜å‚¨æ‰€æœ‰é¢„æµ‹ç»“æœ
            processed_frames = []
            frame_count = 0
            
            # åˆ›å»ºMediaPipeæ¨¡å‹
            with mp_holistic.Holistic(
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            ) as holistic:
                
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    # æ›´æ–°è¿›åº¦
                    frame_count += 1
                    progress = frame_count / total_frames
                    progress_bar.progress(min(progress, 1.0))
                    status_text.text(f"æ­£åœ¨å¤„ç†ç¬¬ {frame_count}/{total_frames} å¸§")
                    
                    # æ£€æµ‹å…³é”®ç‚¹
                    image, results = mediapipe_detection(frame, holistic)
                    
                    # ç»˜åˆ¶å…³é”®ç‚¹
                    if show_video:
                        draw_styled_landmarks(image, results)
                    
                    # æå–å…³é”®ç‚¹å¹¶æ·»åŠ åˆ°åºåˆ—
                    keypoints = extract_keypoints(results)
                    sequence.append(keypoints)
                    sequence = sequence[-30:]  # ä¿æŒæœ€è¿‘30å¸§
                    
                    # å¦‚æœæœ‰æ‰‹éƒ¨å…³é”®ç‚¹å¹¶ä¸”åºåˆ—è¶³å¤Ÿé•¿ï¼Œè¿›è¡Œé¢„æµ‹
                    if (results.left_hand_landmarks or results.right_hand_landmarks) and len(sequence) == 30:
                        try:
                            # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
                            input_data = torch.tensor(
                                np.expand_dims(sequence, axis=0), 
                                dtype=torch.float32
                            )
                            
                            # è¿›è¡Œé¢„æµ‹
                            with torch.no_grad():
                                res = model(input_data)
                            
                            # è·å–é¢„æµ‹ç»“æœ
                            probabilities = torch.softmax(res, dim=1)
                            max_prob, max_idx = torch.max(probabilities, dim=1)
                            
                            pred_class = gestures[max_idx.item()]
                            confidence = max_prob.item() * 100
                            
                            # å­˜å‚¨é¢„æµ‹ç»“æœ
                            predictions_history.append({
                                'frame': frame_count,
                                'class': pred_class,
                                'confidence': confidence,
                                'timestamp': frame_count / fps
                            })
                            
                            # åœ¨å›¾åƒä¸Šæ˜¾ç¤ºé¢„æµ‹ç»“æœ
                            if show_video:
                                cv2.putText(image, f"é¢„æµ‹: {pred_class}", 
                                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                                           1, (0, 255, 0), 2, cv2.LINE_AA)
                                cv2.putText(image, f"ç½®ä¿¡åº¦: {confidence:.1f}%", 
                                           (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 
                                           1, (0, 255, 0), 2, cv2.LINE_AA)
                        
                        except Exception as e:
                            st.warning(f"ç¬¬ {frame_count} å¸§é¢„æµ‹å‡ºé”™: {str(e)}")
                    
                    # ä¿å­˜å¤„ç†åçš„å¸§
                    if show_video:
                        processed_frames.append(image)
                    
                    # å†™å…¥è¾“å‡ºè§†é¢‘
                    if save_output:
                        out.write(image)
            
            # é‡Šæ”¾èµ„æº
            cap.release()
            if save_output:
                out.release()
            
            # æ›´æ–°å®ŒæˆçŠ¶æ€
            progress_bar.progress(1.0)
            status_text.text("âœ… å¤„ç†å®Œæˆï¼")
            
            with col2:
                st.subheader("ğŸ“Š åˆ†æç»“æœ")
                
                # æ‰“å°æ–‡ä»¶åç§°
                st.info(f"**æ–‡ä»¶åç§°:** {file_name}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰é¢„æµ‹ç»“æœ
                if predictions_history:
                    # è·å–æœ€å30å¸§çš„é¢„æµ‹ç»“æœ
                    last_30_predictions = predictions_history[-30:] if len(predictions_history) >= 30 else predictions_history
                    
                    st.success("âœ… æ‰¾åˆ°æ‰‹éƒ¨åŠ¨ä½œï¼")
                    st.write(f"**æ€»é¢„æµ‹å¸§æ•°:** {len(predictions_history)}")
                    
                    # åˆ†ææœ€å30å¸§çš„é¢„æµ‹
                    if last_30_predictions:
                        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å‡ºç°æ¬¡æ•°
                        class_counts = {}
                        for pred in last_30_predictions:
                            pred_class = pred['class']
                            class_counts[pred_class] = class_counts.get(pred_class, 0) + 1
                        
                        # æ‰¾åˆ°å‡ºç°æœ€å¤šçš„ç±»åˆ«
                        if class_counts:
                            most_common_class = max(class_counts.items(), key=lambda x: x[1])
                            final_prediction = most_common_class[0]
                            confidence_score = (most_common_class[1] / len(last_30_predictions)) * 100
                            
                            st.subheader("ğŸ¯ æœ€ç»ˆé¢„æµ‹åˆ†ç±»")
                            st.success(f"**é¢„æµ‹ç»“æœ:** {final_prediction}")
                            st.write(f"**ç½®ä¿¡åº¦:** {confidence_score:.1f}%")
                            st.write(f"**åœ¨æœ€å{len(last_30_predictions)}å¸§ä¸­å‡ºç°æ¬¡æ•°:** {most_common_class[1]}æ¬¡")
                            
                            # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
                            st.subheader("ğŸ“ˆ è¯¦ç»†ç»Ÿè®¡")
                            for cls, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True):
                                percentage = (count / len(last_30_predictions)) * 100
                                st.write(f"- **{cls}**: {count}æ¬¡ ({percentage:.1f}%)")
                        else:
                            st.warning("æœªèƒ½ç¡®å®šæœ€ç»ˆåˆ†ç±»")
                    else:
                        st.warning("æ²¡æœ‰è¶³å¤Ÿçš„é¢„æµ‹æ•°æ®è¿›è¡Œåˆ†ç±»")
                    
                    # æ˜¾ç¤ºé¢„æµ‹å†å²è¡¨æ ¼
                    if len(predictions_history) > 0:
                        st.subheader("ğŸ“‹ é¢„æµ‹å†å²")
                        
                        # åˆ›å»ºç®€åŒ–çš„å†å²è§†å›¾
                        history_data = []
                        for i, pred in enumerate(predictions_history[-20:]):  # åªæ˜¾ç¤ºæœ€å20æ¡
                            history_data.append({
                                'å¸§å·': pred['frame'],
                                'æ—¶é—´æˆ³': f"{pred['timestamp']:.1f}s",
                                'é¢„æµ‹åˆ†ç±»': pred['class'],
                                'ç½®ä¿¡åº¦': f"{pred['confidence']:.1f}%"
                            })
                        
                        st.dataframe(history_data)
                
                else:
                    st.warning("âš ï¸ æœªæ£€æµ‹åˆ°æ‰‹éƒ¨åŠ¨ä½œï¼Œè¯·ç¡®ä¿è§†é¢‘ä¸­åŒ…å«æ¸…æ™°çš„æ‰‹éƒ¨åŠ¨ä½œ")
                
                # æ˜¾ç¤ºå¤„ç†åçš„è§†é¢‘å¸§
                if show_video and processed_frames:
                    st.subheader("ğŸ¥ å¤„ç†åçš„è§†é¢‘é¢„è§ˆ")
                    
                    # é€‰æ‹©æ˜¾ç¤ºä¸€äº›å…³é”®å¸§
                    display_frames = []
                    if len(processed_frames) > 10:
                        step = len(processed_frames) // 9
                        for i in range(0, len(processed_frames), step):
                            if len(display_frames) < 9 and i < len(processed_frames):
                                display_frames.append(processed_frames[i])
                    else:
                        display_frames = processed_frames
                    
                    # æ˜¾ç¤ºå¸§ç½‘æ ¼
                    cols = st.columns(3)
                    for idx, frame in enumerate(display_frames[:9]):
                        with cols[idx % 3]:
                            # è°ƒæ•´å›¾åƒå¤§å°ä»¥é€‚åº”æ˜¾ç¤º
                            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                            img = Image.fromarray(frame_rgb)
                            img.thumbnail((200, 200))
                            st.image(img, caption=f"å¸§ {idx*step if len(processed_frames)>10 else idx+1}")
                
                # ä¸‹è½½å¤„ç†åçš„è§†é¢‘
                if save_output and os.path.exists(output_path):
                    with open(output_path, "rb") as file:
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½å¤„ç†åçš„è§†é¢‘",
                            data=file,
                            file_name=output_path,
                            mime="video/mp4"
                        )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(video_path)
                if save_output and os.path.exists(output_path):
                    os.unlink(output_path)
            except:
                pass

# æ·»åŠ è¯´æ˜
with st.expander("â„¹ï¸ ä½¿ç”¨è¯´æ˜"):
    st.markdown("""
    ### ä½¿ç”¨æ–¹æ³•ï¼š
    1. **ä¸Šä¼ è§†é¢‘**ï¼šç‚¹å‡»"Browse files"ä¸Šä¼ MP4æ ¼å¼çš„è§†é¢‘æ–‡ä»¶
    2. **é…ç½®å‚æ•°**ï¼šåœ¨ä¾§è¾¹æ è®¾ç½®æ‰‹åŠ¿ç±»åˆ«å’Œæ¨¡å‹è·¯å¾„
    3. **å¼€å§‹å¤„ç†**ï¼šç‚¹å‡»"å¼€å§‹å¤„ç†è§†é¢‘"æŒ‰é’®è¿›è¡Œåˆ†æ
    4. **æŸ¥çœ‹ç»“æœ**ï¼šç³»ç»Ÿå°†æ˜¾ç¤ºé¢„æµ‹ç»“æœå’Œå¤„ç†åçš„è§†é¢‘é¢„è§ˆ
    
    ### æ³¨æ„äº‹é¡¹ï¼š
    - ç¡®ä¿è§†é¢‘ä¸­åŒ…å«æ¸…æ™°çš„æ‰‹éƒ¨åŠ¨ä½œ
    - æ‰‹åŠ¿ç±»åˆ«éœ€è¦ä¸è®­ç»ƒæ¨¡å‹æ—¶çš„ç±»åˆ«ä¸€è‡´
    - ç³»ç»Ÿä¼šåˆ†æè§†é¢‘æœ€å30å¸§çš„é¢„æµ‹ç»“æœæ¥ç¡®å®šæœ€ç»ˆåˆ†ç±»
    - å¤„ç†æ—¶é—´å–å†³äºè§†é¢‘é•¿åº¦å’Œè®¡ç®—æœºæ€§èƒ½
    
    ### æŠ€æœ¯è¯´æ˜ï¼š
    - ä½¿ç”¨MediaPipeè¿›è¡Œäººä½“å…³é”®ç‚¹æ£€æµ‹
    - ä½¿ç”¨LSTMç¥ç»ç½‘ç»œè¿›è¡Œæ—¶åºåŠ¨ä½œè¯†åˆ«
    - åˆ†ææœ€å30å¸§çš„é¢„æµ‹ç»“æœæ¥ç¡®å®šæœ€ç»ˆæ‰‹åŠ¿åˆ†ç±»
    """)

# æ·»åŠ é¡µè„š
st.markdown("---")
st.markdown("ğŸ‘‹ **æ‰‹åŠ¿è¯†åˆ«åº”ç”¨** | åŸºäºMediaPipeå’ŒLSTMçš„åŠ¨ä½œè¯†åˆ«ç³»ç»Ÿ")
