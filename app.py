import streamlit as st
import cv2
import numpy as np
import os
import tempfile
import torch
from torch import nn
import mediapipe as mp
from PIL import Image

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ‰‹åŠ¿è¯†åˆ«åº”ç”¨",
    page_icon="ğŸ‘‹",
    layout="wide"
)

# åˆå§‹åŒ–MediaPipe
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

def mediapipe_detection(image, model):
    """å¤„ç†å›¾åƒå¹¶æ£€æµ‹å…³é”®ç‚¹"""
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image.flags.writeable = False
    results = model.process(image)
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    return image, results

def extract_keypoints(results):
    """æå–å…³é”®ç‚¹æ•°æ®"""
    pose = np.array([[res.x, res.y, res.z, res.visibility] 
                     for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)
    lh = np.array([[res.x, res.y, res.z] 
                   for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    rh = np.array([[res.x, res.y, res.z] 
                   for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)
    
    return np.concatenate([pose, lh, rh])

# å®šä¹‰LSTMæ¨¡å‹ç±»
class CustomLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(CustomLSTM, self).__init__()
        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, 64)
        self.fc2 = nn.Linear(64, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 32)
        self.fc5 = nn.Linear(32, 32)
        self.output_layer = nn.Linear(32, num_classes)
    
    def forward(self, x):
        x, _ = self.lstm1(x)
        x, _ = self.lstm2(x)
        x, _ = self.lstm3(x)
        x = torch.relu(self.fc1(x[:, -1, :]))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = torch.relu(self.fc4(x))
        x = torch.relu(self.fc5(x))
        x = self.output_layer(x)
        return x

# æ‰‹åŠ¿ç±»åˆ«åˆ—è¡¨
gestures = np.array(['polis','nasi','abang','apa','hari','ribut','pukul','beli','emak','perlahan'])

# åŠ è½½æ¨¡å‹
model_path = "model.pth"
input_size = 258
hidden_size = 64
num_classes = len(gestures)

try:
    model = torch.load(model_path, weights_only=False)
    model.eval()
    except Exception as e:
    st.error(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
    print(torch.cuda.is_available)
    model = CustomLSTM(input_size, hidden_size, num_classes)
    model.eval()

# ä¸»ç•Œé¢
st.title("ğŸ‘‹ æ‰‹åŠ¿è¯†åˆ«åº”ç”¨")

# ç¬¬ä¸€ä¸ªæ–‡æœ¬æ¡†ï¼šä¸Šä¼ æç¤º
st.markdown("### è¯·ä¸Šä¼ æ‰‹è¯­è§†é¢‘æ¥è·å–é¢„æµ‹")

# ä¸Šä¼ è§†é¢‘
uploaded_file = st.file_uploader("é€‰æ‹©è§†é¢‘æ–‡ä»¶", type=['mp4', 'avi', 'mov'])

if uploaded_file is not None:
    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        video_path = tmp_file.name
    
    # è¯»å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
    cap = cv2.VideoCapture(video_path)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps if fps > 0 else 0
    
    # ç¬¬äºŒä¸ªæ–‡æœ¬æ¡†ï¼šæ˜¾ç¤ºè§†é¢‘ä¿¡æ¯
    st.markdown("### è§†é¢‘ä¿¡æ¯")
    col1, col2 = st.columns(2)
    with col1:
        st.info(f"**æ–‡ä»¶å:** {uploaded_file.name}")
    with col2:
        st.info(f"**æ—¶é•¿:** {duration:.2f}ç§’")
    
    # å¼€å§‹å¤„ç†æŒ‰é’®
    if st.button("å¼€å§‹é¢„æµ‹", type="primary"):
        with st.spinner("æ­£åœ¨å¤„ç†è§†é¢‘..."):
            # åˆå§‹åŒ–å˜é‡
            cap = cv2.VideoCapture(video_path)
            sequence = []
            predictions_history = []
            frame_count = 0
            
            # è¿›åº¦æ¡
            progress_bar = st.progress(0)
            
            # åˆ›å»ºMediaPipeæ¨¡å‹
            with mp_holistic.Holistic(
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            ) as holistic:
                
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    # æ›´æ–°è¿›åº¦
                    frame_count += 1
                    progress = frame_count / total_frames
                    progress_bar.progress(min(progress, 1.0))
                    
                    # æ£€æµ‹å…³é”®ç‚¹
                    _, results = mediapipe_detection(frame, holistic)
                    
                    # æå–å…³é”®ç‚¹å¹¶æ·»åŠ åˆ°åºåˆ—
                    keypoints = extract_keypoints(results)
                    sequence.append(keypoints)
                    sequence = sequence[-30:]  # ä¿æŒæœ€è¿‘30å¸§
                    
                    # å¦‚æœæœ‰æ‰‹éƒ¨å…³é”®ç‚¹å¹¶ä¸”åºåˆ—è¶³å¤Ÿé•¿ï¼Œè¿›è¡Œé¢„æµ‹
                    if (results.left_hand_landmarks or results.right_hand_landmarks) and len(sequence) == 30:
                        try:
                            # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
                            input_data = torch.tensor(
                                np.expand_dims(sequence, axis=0), 
                                dtype=torch.float32
                            )
                            
                            # è¿›è¡Œé¢„æµ‹
                            with torch.no_grad():
                                res = model(input_data)
                            
                            # è·å–é¢„æµ‹ç»“æœ
                            probabilities = torch.softmax(res, dim=1)
                            max_prob, max_idx = torch.max(probabilities, dim=1)
                            
                            pred_class = gestures[max_idx.item()]
                            confidence = max_prob.item() * 100
                            
                            # å­˜å‚¨é¢„æµ‹ç»“æœ
                            predictions_history.append({
                                'class': pred_class,
                                'confidence': confidence
                            })
                            
                        except Exception as e:
                            pass
            
            # é‡Šæ”¾èµ„æº
            cap.release()
            progress_bar.progress(1.0)
            
            # ç¬¬ä¸‰ä¸ªæ–‡æœ¬æ¡†ï¼šæ˜¾ç¤ºé¢„æµ‹ç»“æœ
            st.markdown("### é¢„æµ‹ç»“æœ")
            
            if predictions_history:
                # è·å–æœ€å30å¸§çš„é¢„æµ‹ç»“æœ
                last_30_predictions = predictions_history[-30:] if len(predictions_history) >= 30 else predictions_history
                
                # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å‡ºç°æ¬¡æ•°
                class_counts = {}
                for pred in last_30_predictions:
                    pred_class = pred['class']
                    class_counts[pred_class] = class_counts.get(pred_class, 0) + 1
                
                # æ‰¾åˆ°å‡ºç°æœ€å¤šçš„ç±»åˆ«
                if class_counts:
                    most_common_class = max(class_counts.items(), key=lambda x: x[1])
                    final_prediction = most_common_class[0]
                    confidence_score = (most_common_class[1] / len(last_30_predictions)) * 100
                    
                    st.success(f"**é¢„æµ‹ç»“æœ:** {final_prediction}")
                    st.info(f"**ç½®ä¿¡åº¦:** {confidence_score:.1f}%")
                else:
                    st.warning("æœªèƒ½ç¡®å®šåˆ†ç±»ç»“æœ")
            else:
                st.warning("æœªæ£€æµ‹åˆ°æ‰‹éƒ¨åŠ¨ä½œ")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(video_path)
            except:
                pass

# æ·»åŠ ç®€å•çš„è¯´æ˜
st.markdown("---")
st.markdown("*ä¸Šä¼ åŒ…å«æ‰‹è¯­çš„MP4è§†é¢‘æ–‡ä»¶ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è¯†åˆ«æ‰‹åŠ¿åŠ¨ä½œ*")
